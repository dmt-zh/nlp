{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49430a9-5cc0-4cd6-8c47-e79a4dec94bf",
   "metadata": {},
   "source": [
    "# Представление текста. Часть 2.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Задачи:\n",
    "- **Embeddings**\n",
    "- **Word2Vec**\n",
    "- **SentenceTransformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c8c28a-bce6-4a09-be31-c81ec0342ece",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Эмбеддинг (англ. _embedding_) — это вектор в виде массива чисел, который получается после преобразования текста языковой моделью.\n",
    "\n",
    "В общем виде, эмбеддинги делятся на:\n",
    "- Word Embeddings — слова преобразуют в векторы, так что слова с похожим значением имеют похожие векторные представления.\n",
    "- Sentence Embeddings — создание векторных представлений для целых предложений или даже абзацев, улавливая гораздо более тонкие нюансы языка.\n",
    "\n",
    "В NLP, эмбеддинги слов используются для того, чтобы компьютер мог понять, что слова «кошка» и «котенок» связаны между собой ближе, чем, скажем, «кошка» и «окошко». Это достигается путем присвоения словам векторов, которые отражают их значение и контекстное использование в языке.\n",
    "\n",
    "Векторное представление текста (эмбеддинги) используется для:\n",
    "- улучшения качества поиска — эмбеддинги позволяют оценивать сходство между текстовыми запросами на основе расстояния между соответствующими векторами;\n",
    "- уменьшения размерности данных — с помощью эмбеддингов вы можете представить текстовые запросы в виде числовых векторов, что позволяет снизить размерность данных и ускорить их обработку;\n",
    "- обеспечения универсальности — эмбеддинги можно использовать для различных задач обработки естественного языка, таких как Retrieval Augmented Generation (RAG), классификация текстов, кластеризация и других."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce27efdf-ff48-4c62-8e97-21658221432f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85fa35e-c936-474a-8480-a617218ab176",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "\n",
    "Word2Vec использует нейронные сети для обучения векторных представлений слов из больших наборов текстовых данных. Существуют две основные архитектуры Word2Vec:\n",
    "- **Continuous bag of words (CBOW)**: предсказывает текущее слово на основе контекста (окружающих слов). Например, в предложении \"Собака лает на ___\", CBOW попытается угадать недостающее слово (например, \"почтальона\") на основе окружающих слов.\n",
    "<img src='../images/cbow.jpg' width=\"628\" height=\"428\" >\n",
    "\n",
    "- **Skip-gram**: работает наоборот по сравнению с CBOW. Использует текущее слово для предсказания окружающих его слов в предложении. Например, если взять слово \"кошка\", модель попытается предсказать слова, которые часто встречаются в окружении слова \"кошка\", такие как \"мышь\", \"мяукает\" и т.д.\n",
    "<img src='../images/skip_gram.jpg' width=\"630\" height=\"428\" >\n",
    "\n",
    "<br>\n",
    "\n",
    "Перечень предобученных [**моделей Word2Vec**](https://github.com/piskvorky/gensim-data?tab=readme-ov-file#models)\n",
    "Перечень предобученных [**моделей Word2Vec для русского языка**](https://rusvectores.org/ru/models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f98b30-cf23-4196-9b63-dd7aa7d2d2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed366f7-17b8-4b14-b992-2355511692b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
